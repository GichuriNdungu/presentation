{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujuLMxSvoXbg"
      },
      "source": [
        "# Optimization Techniques in Machine Learning\n",
        "\n",
        "Objective: This assignment aims to explore implementation or Machine Learning Models with regularization, optimization and Error analysisÂ  techniques used in machine learning to improve models' performance, convergence speed, and efficiency..\n",
        "\n",
        "A Notebook detailing the following\n",
        "\n",
        "* Project name\n",
        "* Group members and Roles\n",
        "* Clear out puts from cells\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Instructions**\n",
        "\n",
        "1. Acquire a publicly available dataset suitable for **classification** tasks.\n",
        "2. Implement a simple machine learning model based on neural networks on the chosen dataset without any optimization techniques.\n",
        "3. Implement and compare the model's performance after applying regularization and optimization techniques.\n",
        "4. Compare and contrast L1 regularization (Lasso) and L2 regularization (Ridge). Discuss situations where each type of regularization might be preferred.\n",
        "5. Make predictions using test data (Make sure you have training, validation, and test datasets)\n",
        "6. Submit the code and a detailed explanation of the implementation, including libraries used, parameter settings, and observed results.\n",
        "\n",
        "Submission Guidelines:\n",
        "\n",
        "Link to the slides and notebook will be submitted via Canvas\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8VW_IzbI3od"
      },
      "source": [
        "\n",
        "# Case Study and Implementation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wGCnpzs9M4Fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1FN7bFeIxfH"
      },
      "source": [
        "# The Dataset\n",
        "> ***Brief Description:***\n",
        "State the Problem and A short Description of the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "nas-T7xwPIso",
        "outputId": "9397575f-d364-4c1b-9e1b-cc000d2fe9be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'str'> <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "#TO DO: Load Data (Seprate into: Train, Validation and test sets)\n",
        "# Load training data\n",
        "x_train = './archive4/fashion-mnist_train.csv'\n",
        "\n",
        "data_train = pd.read_csv(x_train)\n",
        "# data.head(5)\n",
        "\n",
        "# Separate features and labels\n",
        "X_train = np.array(data_train.drop('label', axis=1))\n",
        "y_train = np.array(data_train['label'])\n",
        "print(type(x_train), type(y_train))\n",
        "# Split training data into train and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_full, X_val, y_train_full, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# X_train_full, y_train_full is for training the model\n",
        "# X_val, y_val is for validation\n",
        "\n",
        "x_test = './archive4/fashion-mnist_test.csv'\n",
        "data_test = pd.read_csv(x_test)\n",
        "# print(data_test.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE9oQm8OZSvO"
      },
      "source": [
        "# SECTION 1: Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve4AiQmGMzIN"
      },
      "source": [
        "#Task: Model Architecture:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "TODO: Insert an image with the Model architecture here.Replace the image Below\n",
        "```\n",
        "> <img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*v1ohAG82xmU6WGsG2hoE8g.png\" alt=\"?\" style=\"width:25px\"/>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR4BNYoUMzMP"
      },
      "source": [
        "#Task: Create A Model Without any Optimization techniques\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "gGtPmYb_SDHy",
        "outputId": "eae44842-6eb4-4ce2-a84f-1a399dbb2bd4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Input layer with placeholder dimensions for flexibility\n",
        "model_1 = Sequential()\n",
        "model_1.add(Dense(128, activation='relu', input_shape=(None, 784)))\n",
        "\n",
        "# Additional layers (adjust as needed for your architecture)\n",
        "model_1.add(Dense(128, activation='relu'))  # Hidden layer with 128 neurons\n",
        "model_1.add(Dense(1, activation='softmax'))  # Output layer with 10 classes for Fashion MNIST\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "kzcWeOk-Sy9x",
        "outputId": "4ce30d5a-7259-41ba-d2fa-b2dcc7161de3"
      },
      "outputs": [],
      "source": [
        "model_1.compile(loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ9OXp1TSaXn"
      },
      "source": [
        "# Task: Print out the Final Model Accuracy and plot the Loss curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6O11JorTY3z-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "1313/1313 [==============================] - 9s 5ms/step - loss: 0.0000e+00 - accuracy: 0.1001\n",
            "Epoch 2/10\n",
            "1313/1313 [==============================] - 7s 5ms/step - loss: 0.0000e+00 - accuracy: 0.1001\n",
            "Epoch 3/10\n",
            "1313/1313 [==============================] - 7s 5ms/step - loss: 0.0000e+00 - accuracy: 0.1001\n",
            "Epoch 4/10\n",
            "1313/1313 [==============================] - 7s 5ms/step - loss: 0.0000e+00 - accuracy: 0.1001\n",
            "Epoch 5/10\n",
            "1313/1313 [==============================] - 7s 5ms/step - loss: 0.0000e+00 - accuracy: 0.1001\n",
            "Epoch 6/10\n",
            "1313/1313 [==============================] - 7s 5ms/step - loss: 0.0000e+00 - accuracy: 0.1001\n",
            "Epoch 7/10\n",
            "1313/1313 [==============================] - 7s 5ms/step - loss: 0.0000e+00 - accuracy: 0.1001\n",
            "Epoch 8/10\n",
            "1313/1313 [==============================] - 7s 5ms/step - loss: 0.0000e+00 - accuracy: 0.1001\n",
            "Epoch 9/10\n",
            "1313/1313 [==============================] - 7s 5ms/step - loss: 0.0000e+00 - accuracy: 0.1001\n",
            "Epoch 10/10\n",
            "1313/1313 [==============================] - 7s 5ms/step - loss: 0.0000e+00 - accuracy: 0.1001\n"
          ]
        }
      ],
      "source": [
        "history = model_1.fit(X_train_full, y_train_full, epochs=10, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fkihQBsaUxGh"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'val_loss'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m loss \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Plot loss curves\u001b[39;00m\n\u001b[0;32m      5\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(loss) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
          ]
        }
      ],
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Plot loss curves\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the loss curve\n",
        "plot_loss_curve(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hc6k2taT0_n"
      },
      "source": [
        "# SECTION 2: Optimization and Regularization implementation\n",
        "At this point you should now create a model that is more optimized in order to see better perfomance.\n",
        "As done before make sure to plot out the loss curve and the accuracy and loss in verbose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nr6HuVEtXvoP"
      },
      "outputs": [],
      "source": [
        "#TODO:\n",
        "model_2 = pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UygzqjB-Xvgo"
      },
      "outputs": [],
      "source": [
        "#TODO:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NEtnjqxXvXm"
      },
      "outputs": [],
      "source": [
        "#TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aZLEriYOXI1"
      },
      "source": [
        "#Task: Make Predictions using the best saved model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnBLwqTJX3p0"
      },
      "source": [
        "Create a confusion Matrix and F1 score for both Models. Ensure outputs for the cells are visible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO79SOsZYG-M"
      },
      "source": [
        "Finally, Make predictions using the best model. By the time you get to this cell you may realise at some point you needed to save the model so that you cal load it later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqqe2PasUIAG"
      },
      "outputs": [],
      "source": [
        "def make_predictions(model_path, X):\n",
        "\n",
        "    # Load the model\n",
        "    model = load_model(pass)\n",
        "    # Make predictions\n",
        "    predictions = pass\n",
        "    # Convert probabilities to binary labels (0 or 1)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "#Modify the code appropriately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_jwbvaAUMj4"
      },
      "outputs": [],
      "source": [
        "model_path = pass\n",
        "make_predictions(model_path, X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfTHk2nZMzTH"
      },
      "source": [
        "Congratulations!!\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
